{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AU_Hackathon-Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavyashahh/TapuSena/blob/master/AU_Hackathon_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ANwbpVmf0sBQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**author** = \"Levin Dabhi , Bhavya Shah , Heet Shah , Krutarth Bhatt \" version = \"1\" git = \"[TapuSena](https://github.com/bhavyashahh/TapuSena)\"\n"
      ]
    },
    {
      "metadata": {
        "id": "hIwhtG8GiW5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  # Ingenium Hackathon 2019\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "70igVdCmibtv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "***Problem Statement : *** \n",
        "***                                     An application that extracts the title, authors, and abstract from the pdf of a research paper.***\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6yADNdL5idXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Downloading necessary libraries and mounting Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Sfd3_sSz3mqI",
        "colab_type": "code",
        "outputId": "48b2804d-83e1-4a42-835a-ac90f2a9737e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install pytesseract > /dev/null 2>&1\n",
        "!sudo apt install tesseract-ocr > /dev/null 2>&1\n",
        "!pip install tesseract > /dev/null 2>&1\n",
        "!sudo apt install poppler-utils > /dev/null 2>&1\n",
        "!pip install pdf2image  > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U_Z2LAKmi5f6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries "
      ]
    },
    {
      "metadata": {
        "id": "Rh9xsT3r30vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pytesseract as py   #for Optical Character Recognition\n",
        "from PIL import Image      #for manipulating image data\n",
        "import pdf2image           #for manipulating pdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riUPo4DT_yTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# converting pdf to images by libary pdf2images\n",
        " \n",
        "# Here pdf2images function convert from path takes 2 argumnet 1. path of pdf and 2. path where you want to save output images\n",
        "# function returns list of ppm images where each image is correspoing page from pdf\n",
        "\n",
        "\n",
        "pages = pdf2image.convert_from_path(r'/content/gdrive/My Drive/yolo.pdf' , output_folder=r'/content/gdrive/My Drive/dust-bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNOCVDvpjMb0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Performing Optical Character Recognition (OCR)**"
      ]
    },
    {
      "metadata": {
        "id": "qQPRuLfCH-KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text=[None]*(len(pages))\n",
        "\n",
        "\n",
        "# Doing for 1st page only\n",
        "for i in range(len(pages)):\n",
        "    text[i]=py.image_to_string(pages[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4L6kLbnjurq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example text : Page 1 of research paper**"
      ]
    },
    {
      "metadata": {
        "id": "hzcDoUjbfzSf",
        "colab_type": "code",
        "outputId": "14c8adb8-1c51-4224-c40d-6f5aeb3c8e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1836
        }
      },
      "cell_type": "code",
      "source": [
        "# Sampling printing\n",
        "print(text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506.02640v5 [cs.CV] 9 May 2016\n",
            "\n",
            "arXiv\n",
            "\n",
            "You Only Look Once:\n",
            "Unified, Real-Time Object Detection\n",
            "\n",
            "Joseph Redmon*, Santosh Divvala*', Ross Girshick‘, Ali Farhadi*t\n",
            "University of Washington”, Allen Institute for Al', Facebook AI Research!\n",
            "http://pjreddie.com/yolo/\n",
            "\n",
            "Abstract\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "\n",
            "Current detection systems repurpose classifiers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classifier for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classifier is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "\n",
            "More recent approaches like R-CNN use region proposal\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "1. Resize image.\n",
            "2. Run convolutional network.\n",
            "3. Non-max suppression.\n",
            "\n",
            " \n",
            "\n",
            "Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448 x 448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s confidence.\n",
            "\n",
            "methods to first generate potential bounding boxes in an im-\n",
            "age and then run a classifier on these proposed boxes. After\n",
            "classification, post-processing is used to refine the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This unified model has several benefits\n",
            "over traditional methods of object detection.\n",
            "\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n",
            "GPU and a fast version runs at more than 150 fps. This\n",
            "means we can process streaming video in real-time with\n",
            "less than 25 milliseconds of latency. Furthermore, YOLO\n",
            "achieves more than twice the mean average precision of\n",
            "other real-time systems. For a demo of our system running\n",
            "in real-time on a webcam please see our project webpage:\n",
            "http://pjreddie.com/yolo/.\n",
            "\n",
            "Second, YOLO reasons globally about the image when\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_NyHjmIQj4Le",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**For extracting Title from Research Paper**\n",
        "\n",
        "\n",
        "> Works on every type of research paper\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EDeEPOY-fp_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Finding title here\n",
        "sentence=text[0]\n",
        "i_arxiv=sentence.find('arXiv')\n",
        "title=['']*100\n",
        "if(i_arxiv==-1):\n",
        "    i=0\n",
        "    while(sentence[i]!= '\\n' and sentence[i+1]!= '\\n'):\n",
        "        title[i]=sentence[i]\n",
        "        i=i+1\n",
        "else:\n",
        "    i=i_arxiv+7\n",
        "    j=0\n",
        "    while(sentence[i]!='\\n' and sentence[i]!='\\n' and sentence[i+1]!='\\n'):\n",
        "        title[j]=sentence[i]\n",
        "        i=i+1\n",
        "        j=j+1\n",
        "        if(i==len(sentence)):\n",
        "            break\n",
        "title_end=i                                          #title end shows where title ends "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bCTxUUwke8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Displaying Title"
      ]
    },
    {
      "metadata": {
        "id": "SDr0ZOP4wQn3",
        "colab_type": "code",
        "outputId": "4cb62c41-d1a6-43c4-9452-d737bd870ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "title=''.join(title)\n",
        "print(\"Title :\" ,title)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title : You Only Look Once\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbrgh-lkl2Na",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**For extraction of authors :**\n",
        "> Approach : \n",
        "\n",
        "> 1.Named Entity recognition through NLTK library\n",
        "\n",
        "> 2.Named Entity recognition through through SPAcy library\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Mslxa6zwmGCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Approach 1 . Through NLTK"
      ]
    },
    {
      "metadata": {
        "id": "mkC9ZDJMr0Uj",
        "colab_type": "code",
        "outputId": "5a859ef0-9770-4dd8-d604-025685698d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk                                              #importing nltk and named entitiy \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "aN8rew4Drvt7",
        "colab_type": "code",
        "outputId": "024d776c-ff23-4c87-d65e-94eea6fbe5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "sentence = text[0]\n",
        "print(\"Author :\")\n",
        "for sent in nltk.sent_tokenize(sentence):\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            #print(type(chunk))\n",
        "            if (chunk.label()=='PERSON'):\n",
        "                print(chunk.label() , ': ', ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author :\n",
            "PERSON :  Object Detection Joseph\n",
            "PERSON :  Santosh Divvala*\n",
            "PERSON :  Ross Girshick\n",
            "PERSON :  Ali\n",
            "PERSON :  Allen Institute\n",
            "PERSON :  Al\n",
            "PERSON :  Facebook AI Research\n",
            "PERSON :  Fast YOLO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYCk570FmsUK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apporach 2:"
      ]
    },
    {
      "metadata": {
        "id": "PbyG7Jcfezj0",
        "colab_type": "code",
        "outputId": "393217f5-b512-4c7b-a711-e2d81ad8f987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "authors=''\n",
        "doc = nlp(sentence)\n",
        "i=0\n",
        "for X in doc.ents:\n",
        "    if(X.label_=='PERSON'):\n",
        "        if(X.text[0]!='\\n'):\n",
        "            authors+=X.text\n",
        "            authors+=','\n",
        "            authors+=\" \"\n",
        "print(\"Authors:\")\n",
        "print(authors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authors:\n",
            "Joseph Redmon, Santosh Divvala, Ross Girshick‘, Allen Institute, YOLO, YOLO, YOLO\n",
            ", \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gnM8yX-0m2NE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "\n",
        "\n",
        "> Here NLTK is less noisy but more accurate .\n",
        "\n",
        ">Accuracy in SPAcy is very high but its answer is liitle noisy\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yF4STDeAk3rU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**For extracting Abstract from Research Paper**\n",
        ">Mostly works on all research paper"
      ]
    },
    {
      "metadata": {
        "id": "srIfPIE9BTWW",
        "colab_type": "code",
        "outputId": "d43bce75-3d88-4bd0-d0be-151f1d70e3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "i_abstract=text[0].find('Abstract')\n",
        "if(i_abstract==-1):\n",
        "    i_abstract=word.find('ABSTRACT')\n",
        "    if(i_abstract==-1):\n",
        "        print(\"<<<<<<<<-----Abstract not found in paper----->>>>>>>\")\n",
        "# print(i_abstract)\n",
        "        \n",
        "        \n",
        "abstract=['']*15000     \n",
        "    \n",
        "    \n",
        "        \n",
        "if(i_abstract!=-1):\n",
        "    i_intro=text[0].find('Introduction')\n",
        "    if(i_intro==-1):\n",
        "        i_intro=text[0].find('INTRODUCTION')\n",
        "            \n",
        "i_intro=i_intro-3           \n",
        "i=0\n",
        "if(i_intro!=-1):\n",
        "    for j in range(i_abstract+9,i_intro,1):\n",
        "        abstract[i]=text[0][j]\n",
        "        i=i+1\n",
        "    \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "abstract=''.join(abstract)\n",
        "print(\"Abstract  :\" , abstract)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract  : \n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rebKEzeto-th",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**All three Title , Author and Abstract works on research paper of various domain**"
      ]
    },
    {
      "metadata": {
        "id": "hmhgvm2Fp28O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Output in JSON form**"
      ]
    },
    {
      "metadata": {
        "id": "iqzRei74o18p",
        "colab_type": "code",
        "outputId": "d5dffe53-dc00-4f04-fedf-f4c18282eaba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "print(json.dumps({\"Title\":title,\"Author/s\":authors,\"Abstract\":abstract}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"Title\": \"You Only Look Once\", \"Author/s\": \"Joseph Redmon, Santosh Divvala, Ross Girshick\\u2018, Allen Institute, YOLO, YOLO, YOLO\\n, \", \"Abstract\": \"\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classifiers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\n\\nOur unified architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nProcesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n\\n\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ooCRKSomqvE3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Extracting related work**"
      ]
    },
    {
      "metadata": {
        "id": "TEXu6Lx3qlLI",
        "colab_type": "code",
        "outputId": "3116a5f4-d3d7-4888-ba15-44a0d10ff5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "k=-1\n",
        "for i in range(len(pages)):\n",
        "    word = text[i]\n",
        "    r1 = word.find('Related Work')\n",
        "    if(r1==-1):\n",
        "        r1 = word.find('RELATED WORK')\n",
        "    if(r1>0):\n",
        "        print(r1)\n",
        "        k=i\n",
        "        break\n",
        "    \n",
        "if(k==-1):\n",
        "    print('Related Work not found!!!')\n",
        "elif(k>0):\n",
        "    rw = []\n",
        "    no = int(text[k][r1-2])\n",
        "    no += 1\n",
        "    no = str(no)\n",
        "    r1 += 13\n",
        "    i=r1\n",
        "    while(True):\n",
        "        if(text[k][i]==no and text[k][i-1]=='\\n'):\n",
        "            break\n",
        "        else:\n",
        "            rw.append(text[k][i])\n",
        "            i+=1\n",
        "            if(i==len(text[k])):\n",
        "                k+=1\n",
        "                i=0\n",
        "    rw = ''.join(rw)\n",
        "    print(rw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Related Work not found!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xDTRwOHRsyrb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Extracting keywords**\n",
        ">Apporach :\n",
        "\n",
        ">By calculating frequencies of words and eliminating words having less frequency than threshold.\n",
        "\n",
        ">Words like his/her/he/she/the etc are eliminated"
      ]
    },
    {
      "metadata": {
        "id": "7OPzwH6lq2B8",
        "colab_type": "code",
        "outputId": "fb2570b3-6a03-4442-fbde-4be18fc910fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "string=''\n",
        "data=text\n",
        "print(type(data))\n",
        "for i in range(len(pages)):\n",
        "    w=str(data[i])\n",
        "    w= w.encode('ascii','ignore').lower() #Lowercasing each word\n",
        "    string+=str(w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "20a74gFMts12",
        "colab_type": "code",
        "outputId": "fbfc22a9-bde9-4ab5-d8f4-e6fcf12dab0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import PyPDF2\n",
        "#import textract\n",
        "import re\n",
        "#for i in range(len(pages)):\n",
        "  #print(type(data[0])) \n",
        "  #text_new=''.join(w)\n",
        "#print(type(text_new))  \n",
        "keywords = re.findall(r'[a-zA-Z]\\w+',string)\n",
        "len(keywords) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "n7vaFh79t19U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(set(keywords)),columns=['keywords'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7PUf4UategQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Calculating Weightage**\n",
        ">In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
        "\n",
        ">**TF:**\n",
        ">Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
        "\n",
        ">**TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)**\n",
        "\n",
        ">   **IDF:**\n",
        "    Inverse Document Frequency, which measures how important a term is. While computing TF, all      terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
        "\n",
        ">**IDF(t) = log_e(Total number of documents / Number of documents with term t in it)**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pbZvEKYZuCT7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weightage(word,text,number_of_documents=1):\n",
        "    word_list = re.findall(word,text)\n",
        "    number_of_times_word_appeared =len(word_list)\n",
        "    tf = number_of_times_word_appeared/float(len(text))\n",
        "    idf = np.log((number_of_documents)/float(number_of_times_word_appeared))\n",
        "    tf_idf = tf*idf\n",
        "    return number_of_times_word_appeared,tf,idf ,tf_idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcoJHdBIt5vN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['number_of_times_word_appeared'] = df['keywords'].apply(lambda x: weightage(x,string)[0])\n",
        "df['tf'] = df['keywords'].apply(lambda x: weightage(x,string))\n",
        "df['idf'] = df['keywords'].apply(lambda x: weightage(x,string))\n",
        "df['tf_idf'] = df['keywords'].apply(lambda x: weightage(x,string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bj8qIhvEuJiP",
        "colab_type": "code",
        "outputId": "fd27829c-7ee5-46e8-e2f9-eb8107f67b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "cell_type": "code",
      "source": [
        "df = df.sort_values('tf_idf',ascending=True)\n",
        "df.to_csv('Keywords.csv')\n",
        "df.head(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "      <th>number_of_times_word_appeared</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf_idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nmentation</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>803</th>\n",
              "      <td>limitations</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>yolos</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>nlem</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>website</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>nuniversity</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>dean</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>evaluating</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>above</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>nhelps</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>deng</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>assistive</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>concur</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>boat</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>nsixth</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>root</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>diverges</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>frameworks</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>deviations</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>photographs</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>extracting</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>n63</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>ncomplex</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>graspable</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>ijnter</td>\n",
              "      <td>1</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "      <td>(1, 2.2957895220166217e-05, 0.0, 0.0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         keywords  number_of_times_word_appeared  \\\n",
              "0      nmentation                              1   \n",
              "803   limitations                              1   \n",
              "804         yolos                              1   \n",
              "806          nlem                              1   \n",
              "808       website                              1   \n",
              "809   nuniversity                              1   \n",
              "811          dean                              1   \n",
              "812    evaluating                              1   \n",
              "1530        above                              1   \n",
              "818        nhelps                              1   \n",
              "820          deng                              1   \n",
              "823     assistive                              1   \n",
              "825        concur                              1   \n",
              "800          boat                              1   \n",
              "827        nsixth                              1   \n",
              "831          root                              1   \n",
              "832      diverges                              1   \n",
              "833    frameworks                              1   \n",
              "834    deviations                              1   \n",
              "835   photographs                              1   \n",
              "836    extracting                              1   \n",
              "837           n63                              1   \n",
              "838      ncomplex                              1   \n",
              "840     graspable                              1   \n",
              "841        ijnter                              1   \n",
              "\n",
              "                                         tf  \\\n",
              "0     (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "803   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "804   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "806   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "808   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "809   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "811   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "812   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "1530  (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "818   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "820   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "823   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "825   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "800   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "827   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "831   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "832   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "833   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "834   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "835   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "836   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "837   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "838   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "840   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "841   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "\n",
              "                                        idf  \\\n",
              "0     (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "803   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "804   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "806   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "808   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "809   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "811   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "812   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "1530  (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "818   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "820   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "823   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "825   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "800   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "827   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "831   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "832   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "833   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "834   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "835   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "836   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "837   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "838   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "840   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "841   (1, 2.2957895220166217e-05, 0.0, 0.0)   \n",
              "\n",
              "                                     tf_idf  \n",
              "0     (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "803   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "804   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "806   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "808   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "809   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "811   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "812   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "1530  (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "818   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "820   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "823   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "825   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "800   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "827   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "831   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "832   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "833   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "834   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "835   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "836   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "837   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "838   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "840   (1, 2.2957895220166217e-05, 0.0, 0.0)  \n",
              "841   (1, 2.2957895220166217e-05, 0.0, 0.0)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}