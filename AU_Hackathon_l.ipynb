{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AU_Hackathon-l.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavyashahh/TapuSena/blob/master/AU_Hackathon_l.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Sfd3_sSz3mqI",
        "colab_type": "code",
        "outputId": "59055c98-c4b0-46fb-8989-b58f8ee71bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install pytesseract > /dev/null 2>&1\n",
        "!sudo apt install tesseract-ocr > /dev/null 2>&1\n",
        "!pip install tesseract > /dev/null 2>&1\n",
        "!sudo apt install poppler-utils > /dev/null 2>&1\n",
        "!pip install pdf2image  > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rh9xsT3r30vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pytesseract as py\n",
        "from PIL import Image\n",
        "import pdf2image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riUPo4DT_yTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# converting pdf to images by libary pdf2images\n",
        " \n",
        "# Here pdf2images function convert from path takes 2 argumnet 1. path of pdf and 2. path where you want to save output images\n",
        "# function returns list of ppm images where each image is correspoing page from pdf\n",
        "\n",
        "\n",
        "pages = pdf2image.convert_from_path(r'/content/gdrive/My Drive/yolo.pdf' , output_folder=r'/content/gdrive/My Drive/dust-bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQPRuLfCH-KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install pytesseract > /dev/null 2>&1\n",
        "#doing OCR \n",
        "text=[None]*(len(pages))\n",
        "\n",
        "\n",
        "# Doing for 1st page only\n",
        "for i in range(1):\n",
        "    text[i]=py.image_to_string(pages[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzcDoUjbfzSf",
        "colab_type": "code",
        "outputId": "8260093c-0965-4971-b64d-e505b1e602bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1836
        }
      },
      "cell_type": "code",
      "source": [
        "# Sampling printing\n",
        "print(text[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506.02640v5 [cs.CV] 9 May 2016\n",
            "\n",
            "arXiv\n",
            "\n",
            "You Only Look Once:\n",
            "Unified, Real-Time Object Detection\n",
            "\n",
            "Joseph Redmon*, Santosh Divvala*', Ross Girshick‘, Ali Farhadi*t\n",
            "University of Washington”, Allen Institute for Al', Facebook AI Research!\n",
            "http://pjreddie.com/yolo/\n",
            "\n",
            "Abstract\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "\n",
            "Current detection systems repurpose classifiers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classifier for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classifier is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "\n",
            "More recent approaches like R-CNN use region proposal\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "1. Resize image.\n",
            "2. Run convolutional network.\n",
            "3. Non-max suppression.\n",
            "\n",
            " \n",
            "\n",
            "Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448 x 448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s confidence.\n",
            "\n",
            "methods to first generate potential bounding boxes in an im-\n",
            "age and then run a classifier on these proposed boxes. After\n",
            "classification, post-processing is used to refine the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This unified model has several benefits\n",
            "over traditional methods of object detection.\n",
            "\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n",
            "GPU and a fast version runs at more than 150 fps. This\n",
            "means we can process streaming video in real-time with\n",
            "less than 25 milliseconds of latency. Furthermore, YOLO\n",
            "achieves more than twice the mean average precision of\n",
            "other real-time systems. For a demo of our system running\n",
            "in real-time on a webcam please see our project webpage:\n",
            "http://pjreddie.com/yolo/.\n",
            "\n",
            "Second, YOLO reasons globally about the image when\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mkC9ZDJMr0Uj",
        "colab_type": "code",
        "outputId": "e77d2da8-e265-4df3-ff15-7ec4b2e2b95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "2uXDRI34eH7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Revised Named Entity Recognition using NLTK\n",
        "sentence = text[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aN8rew4Drvt7",
        "colab_type": "code",
        "outputId": "1b6c9864-58a1-4483-def9-1209b0f12e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "for sent in nltk.sent_tokenize(sentence):\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            #print(type(chunk))\n",
        "            print(\"Author :\")\n",
        "            if (chunk.label()=='PERSON'):\n",
        "                print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON Object Detection Joseph\n",
            "PERSON Santosh Divvala*\n",
            "PERSON Ross Girshick\n",
            "PERSON Ali\n",
            "PERSON Allen Institute\n",
            "PERSON Al\n",
            "PERSON Facebook AI Research\n",
            "PERSON Fast YOLO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PbyG7Jcfezj0",
        "colab_type": "code",
        "outputId": "0fce265e-c066-49de-8000-8aafe4155833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "# doing Revised Named Entity recognation using spacy \n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "doc = nlp(sentence)\n",
        "print(\"Author :\")\n",
        "for X in doc.ents :\n",
        "    if X.label_=='PERSON' :\n",
        "        print(X.text)\n",
        "# type(doc)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author :\n",
            "Joseph Redmon\n",
            "Santosh Divvala\n",
            "Ross Girshick‘\n",
            "Allen Institute\n",
            "\n",
            "\n",
            "Abstract\n",
            "\n",
            "\n",
            "YOLO\n",
            "YOLO\n",
            "YOLO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8kWMu8cirjj",
        "colab_type": "code",
        "outputId": "03ee27c5-106a-4a06-ad02-6965a951f148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1853
        }
      },
      "cell_type": "code",
      "source": [
        "print(sentence)\n",
        "print(len(text[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506.02640v5 [cs.CV] 9 May 2016\n",
            "\n",
            "arXiv\n",
            "\n",
            "You Only Look Once:\n",
            "Unified, Real-Time Object Detection\n",
            "\n",
            "Joseph Redmon*, Santosh Divvala*', Ross Girshick‘, Ali Farhadi*t\n",
            "University of Washington”, Allen Institute for Al', Facebook AI Research!\n",
            "http://pjreddie.com/yolo/\n",
            "\n",
            "Abstract\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "\n",
            "Current detection systems repurpose classifiers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classifier for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classifier is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "\n",
            "More recent approaches like R-CNN use region proposal\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "1. Resize image.\n",
            "2. Run convolutional network.\n",
            "3. Non-max suppression.\n",
            "\n",
            " \n",
            "\n",
            "Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448 x 448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s confidence.\n",
            "\n",
            "methods to first generate potential bounding boxes in an im-\n",
            "age and then run a classifier on these proposed boxes. After\n",
            "classification, post-processing is used to refine the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This unified model has several benefits\n",
            "over traditional methods of object detection.\n",
            "\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n",
            "GPU and a fast version runs at more than 150 fps. This\n",
            "means we can process streaming video in real-time with\n",
            "less than 25 milliseconds of latency. Furthermore, YOLO\n",
            "achieves more than twice the mean average precision of\n",
            "other real-time systems. For a demo of our system running\n",
            "in real-time on a webcam please see our project webpage:\n",
            "http://pjreddie.com/yolo/.\n",
            "\n",
            "Second, YOLO reasons globally about the image when\n",
            "4413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDeEPOY-fp_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Finding title here\n",
        "i_arxiv=sentence.find('arXiv')\n",
        "# print(i_arxiv+5)\n",
        "# print(len(sentence))\n",
        "title=['']*100\n",
        "# print(len(title))\n",
        "if(i_arxiv==-1):\n",
        "    i=0\n",
        "    while(sentence[i]!= '\\n' and sentence[i+1]!= '\\n'):\n",
        "        title[i]=sentence[i]\n",
        "        i=i+1\n",
        "else:\n",
        "    i=i_arxiv+7\n",
        "    j=0\n",
        "    while(sentence[i]!='\\n' and sentence[i]!='\\n' and sentence[i+1]!='\\n'):\n",
        "        title[j]=sentence[i]\n",
        "        i=i+1\n",
        "        j=j+1\n",
        "        if(i==len(sentence)):\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDr0ZOP4wQn3",
        "colab_type": "code",
        "outputId": "11cb9cbb-5fbd-4694-e2d0-0d95f465392b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "title=''.join(title)\n",
        "print(\"Title :\" ,title)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title : You Only Look Once\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srIfPIE9BTWW",
        "colab_type": "code",
        "outputId": "09bb0bbb-e1f8-4d7a-cc58-4164192af6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "# Finding abstract here  \n",
        "i_abstract=text[0].find('Abstract')\n",
        "if(i_abstract==-1):\n",
        "    i_abstract=word.find('ABSTRACT')\n",
        "    if(i_abstract==-1):\n",
        "        print(\"<<<<<<<<-----Abstract not found in paper----->>>>>>>\")\n",
        "# print(i_abstract)\n",
        "        \n",
        "        \n",
        "abstract=['']*15000     \n",
        "    \n",
        "    \n",
        "        \n",
        "if(i_abstract!=-1):\n",
        "    i_intro=text[0].find('Introduction')\n",
        "    if(i_intro==-1):\n",
        "        i_intro=text[0].find('INTRODUCTION')\n",
        "            \n",
        "i_intro=i_intro-3           \n",
        "i=0\n",
        "if(i_intro!=-1):\n",
        "    for j in range(i_abstract+9,i_intro,1):\n",
        "        abstract[i]=text[0][j]\n",
        "        i=i+1\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "abstract=''.join(abstract)\n",
        "print(\"Abstract  :\" , abstract)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract  : \n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7rHlz7HeGDHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}