{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AU_Hackathon-l.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavyashahh/TapuSena/blob/master/AU_Hackathon_l.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Sfd3_sSz3mqI",
        "colab_type": "code",
        "outputId": "d5091b40-75de-4d2d-d3ab-943bcfe7b224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install pytesseract > /dev/null 2>&1\n",
        "!sudo apt install tesseract-ocr > /dev/null 2>&1\n",
        "!pip install tesseract > /dev/null 2>&1\n",
        "!sudo apt install poppler-utils > /dev/null 2>&1\n",
        "!pip install pdf2image  > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rh9xsT3r30vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pytesseract as py\n",
        "from PIL import Image\n",
        "import pdf2image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riUPo4DT_yTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#converting pdf to images by libary pdf2images\n",
        "pages = pdf2image.convert_from_path(r'/content/gdrive/My Drive/yolo.pdf' , output_folder=r'/content/gdrive/My Drive/dust-bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PSLuWxcEH7dj",
        "colab_type": "code",
        "outputId": "20b5c9d6-d9f3-4f27-cf67-9006e8e3ab43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(pages))\n",
        "print(len(pages))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qQPRuLfCH-KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install pytesseract > /dev/null 2>&1\n",
        "#doing OCR \n",
        "text=[None]*(len(pages))\n",
        "i=0\n",
        "for i in range(1):\n",
        "    text[i]=py.image_to_string(pages[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzcDoUjbfzSf",
        "colab_type": "code",
        "outputId": "bfca3d31-33d1-435a-b86c-5068ead2b686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1836
        }
      },
      "cell_type": "code",
      "source": [
        "print(text[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506.02640v5 [cs.CV] 9 May 2016\n",
            "\n",
            "arXiv\n",
            "\n",
            "You Only Look Once:\n",
            "Unified, Real-Time Object Detection\n",
            "\n",
            "Joseph Redmon*, Santosh Divvala*', Ross Girshick‘, Ali Farhadi*t\n",
            "University of Washington”, Allen Institute for Al', Facebook AI Research!\n",
            "http://pjreddie.com/yolo/\n",
            "\n",
            "Abstract\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "\n",
            "Current detection systems repurpose classifiers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classifier for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classifier is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "\n",
            "More recent approaches like R-CNN use region proposal\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "1. Resize image.\n",
            "2. Run convolutional network.\n",
            "3. Non-max suppression.\n",
            "\n",
            " \n",
            "\n",
            "Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448 x 448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s confidence.\n",
            "\n",
            "methods to first generate potential bounding boxes in an im-\n",
            "age and then run a classifier on these proposed boxes. After\n",
            "classification, post-processing is used to refine the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This unified model has several benefits\n",
            "over traditional methods of object detection.\n",
            "\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n",
            "GPU and a fast version runs at more than 150 fps. This\n",
            "means we can process streaming video in real-time with\n",
            "less than 25 milliseconds of latency. Furthermore, YOLO\n",
            "achieves more than twice the mean average precision of\n",
            "other real-time systems. For a demo of our system running\n",
            "in real-time on a webcam please see our project webpage:\n",
            "http://pjreddie.com/yolo/.\n",
            "\n",
            "Second, YOLO reasons globally about the image when\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uXDRI34eH7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Revised Named Entity Recognition using NLTK\n",
        "sentence = text[0]\n",
        "#converting char array into string \n",
        "\n",
        "#here sentence is array of content upto Abstract \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkC9ZDJMr0Uj",
        "colab_type": "code",
        "outputId": "10df47fc-ccba-47ac-f61c-d6ae5fc6f248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "aN8rew4Drvt7",
        "colab_type": "code",
        "outputId": "f63658dd-f944-459c-84ea-b260002b5aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "for sent in nltk.sent_tokenize(sentence):\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            #print(type(chunk))\n",
        "            if (chunk.label()=='PERSON'):\n",
        "                print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PERSON Object Detection Joseph\n",
            "PERSON Santosh Divvala*\n",
            "PERSON Ross Girshick\n",
            "PERSON Ali\n",
            "PERSON Allen Institute\n",
            "PERSON Al\n",
            "PERSON Facebook AI Research\n",
            "PERSON Fast YOLO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PbyG7Jcfezj0",
        "colab_type": "code",
        "outputId": "485d1507-1a99-4792-d688-4a2121276446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "doc = nlp(sentence)\n",
        "print([(X.text, X.label_) for X in doc.ents])\n",
        "type(doc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('CV', 'ORG'), ('9', 'CARDINAL'), ('May 2016', 'DATE'), ('\\n', 'GPE'), ('Unified', 'ORG'), ('Real-Time Object Detection', 'ORG'), ('Joseph Redmon', 'PERSON'), ('Santosh Divvala', 'PERSON'), ('Ross Girshick‘', 'PERSON'), ('Ali Farhadi*t\\nUniversity of Washington', 'ORG'), ('Allen Institute', 'PERSON'), (\"Al'\", 'ORG'), ('Facebook AI Research', 'ORG'), ('\\n', 'GPE'), ('\\n\\nAbstract\\n\\n', 'PERSON'), ('YOLO', 'PERSON'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('YOLO', 'ORG'), ('45', 'CARDINAL'), ('\\n', 'GPE'), ('second', 'ORDINAL'), ('155', 'CARDINAL'), ('second', 'ORDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('YOLO\\n', 'ORG'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('DPM', 'ORG'), ('CNN', 'ORG'), ('\\n', 'GPE'), ('1', 'CARDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('DPM', 'ORG'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('10', 'CARDINAL'), ('CNN', 'ORG'), ('1', 'CARDINAL'), ('2', 'CARDINAL'), ('\\n', 'GPE'), ('3', 'CARDINAL'), ('1', 'CARDINAL'), ('The YOLO Detection System', 'ORG'), ('\\n', 'GPE'), ('YOLO', 'ORG'), ('1', 'CARDINAL'), ('\\n', 'GPE'), ('448', 'CARDINAL'), ('448', 'CARDINAL'), ('2', 'CARDINAL'), ('\\n', 'GPE'), ('3', 'CARDINAL'), ('\\n', 'GPE'), ('first', 'ORDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('13', 'CARDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('YOLO', 'PERSON'), ('1', 'CARDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('YOLO', 'ORG'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('First', 'ORDINAL'), ('YOLO', 'ORG'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('45', 'CARDINAL'), ('\\n', 'GPE'), ('second', 'ORDINAL'), ('GPU', 'ORG'), ('more than 150', 'CARDINAL'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('less than 25 milliseconds', 'QUANTITY'), ('YOLO\\n', 'PERSON'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('\\n', 'GPE'), ('Second', 'ORDINAL'), ('YOLO', 'ORG')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "z8kWMu8cirjj",
        "colab_type": "code",
        "outputId": "03ee27c5-106a-4a06-ad02-6965a951f148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1853
        }
      },
      "cell_type": "code",
      "source": [
        "print(sentence)\n",
        "print(len(text[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1506.02640v5 [cs.CV] 9 May 2016\n",
            "\n",
            "arXiv\n",
            "\n",
            "You Only Look Once:\n",
            "Unified, Real-Time Object Detection\n",
            "\n",
            "Joseph Redmon*, Santosh Divvala*', Ross Girshick‘, Ali Farhadi*t\n",
            "University of Washington”, Allen Institute for Al', Facebook AI Research!\n",
            "http://pjreddie.com/yolo/\n",
            "\n",
            "Abstract\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "\n",
            "Current detection systems repurpose classifiers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classifier for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classifier is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "\n",
            "More recent approaches like R-CNN use region proposal\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            "1. Resize image.\n",
            "2. Run convolutional network.\n",
            "3. Non-max suppression.\n",
            "\n",
            " \n",
            "\n",
            "Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448 x 448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s confidence.\n",
            "\n",
            "methods to first generate potential bounding boxes in an im-\n",
            "age and then run a classifier on these proposed boxes. After\n",
            "classification, post-processing is used to refine the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This unified model has several benefits\n",
            "over traditional methods of object detection.\n",
            "\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n",
            "GPU and a fast version runs at more than 150 fps. This\n",
            "means we can process streaming video in real-time with\n",
            "less than 25 milliseconds of latency. Furthermore, YOLO\n",
            "achieves more than twice the mean average precision of\n",
            "other real-time systems. For a demo of our system running\n",
            "in real-time on a webcam please see our project webpage:\n",
            "http://pjreddie.com/yolo/.\n",
            "\n",
            "Second, YOLO reasons globally about the image when\n",
            "4413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDeEPOY-fp_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i_arxiv=sentence.find('arXiv')\n",
        "# print(i_arxiv+5)\n",
        "# print(len(sentence))\n",
        "title=['']*100\n",
        "# print(len(title))\n",
        "if(i_arxiv==-1):\n",
        "    i=0\n",
        "    while(sentence[i]!= '\\n' and sentence[i+1]!= '\\n'):\n",
        "        title[i]=sentence[i]\n",
        "        i=i+1\n",
        "else:\n",
        "    i=i_arxiv+7\n",
        "    j=0\n",
        "    while(sentence[i]!='\\n' and sentence[i]!='\\n' and sentence[i+1]!='\\n'):\n",
        "        title[j]=sentence[i]\n",
        "        i=i+1\n",
        "        j=j+1\n",
        "        if(i==len(sentence)):\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDr0ZOP4wQn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7441793c-a19c-42c9-b6b2-99a8bd6d5af2"
      },
      "cell_type": "code",
      "source": [
        "title=''.join(title)\n",
        "print(title)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You Only Look Once\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HWRkJTznGHWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import arange "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srIfPIE9BTWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "50d6f842-9003-42f6-eb1d-e85ad7491945"
      },
      "cell_type": "code",
      "source": [
        "i_abstract=text[0].find('Abstract')\n",
        "if(i_abstract==-1):\n",
        "    i_abstract=word.find('ABSTRACT')\n",
        "    if(i_abstract==-1):\n",
        "        print(\"<<<<<<<<-----Abstract not found in paper----->>>>>>>\")\n",
        "print(i_abstract)\n",
        "        \n",
        "        \n",
        "abstract=['']*15000     \n",
        "    \n",
        "    \n",
        "        \n",
        "if(i_abstract!=-1):\n",
        "    i_intro=text[0].find('Introduction')\n",
        "    if(i_intro==-1):\n",
        "        i_intro=text[0].find('INTRODUCTION')\n",
        "            \n",
        "i_intro=i_intro-3           \n",
        "print(i_intro)           \n",
        "print(text[0][i_abstract+9])\n",
        "print(i_abstract)\n",
        "i=0\n",
        "for i in range(i_abstract,i_abstract+9):\n",
        "    print(text[0][i])\n",
        "if(i_intro!=-1):\n",
        "    for j in range(i_abstract+9,i_intro,1):\n",
        "        abstract[i]=text[0][j]\n",
        "        i=i+1\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "abstract=''.join(abstract)\n",
        "print(abstract)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "263\n",
            "1406\n",
            "\n",
            "\n",
            "263\n",
            "A\n",
            "b\n",
            "s\n",
            "t\n",
            "r\n",
            "a\n",
            "c\n",
            "t\n",
            "\n",
            "\n",
            "\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classifiers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "\n",
            "Our unified architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "Processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7rHlz7HeGDHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}